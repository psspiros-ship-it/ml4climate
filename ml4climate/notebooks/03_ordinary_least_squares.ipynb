{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef7bc077",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Ordinary Least Squares\n",
    "\n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/git/https%3A%2F%2Fgitlab.in2p3.fr%2Fenergy4climate%2Fpublic%2Feducation%2Fmachine_learning_for_climate_and_energy/master?filepath=book%2Fnotebooks%2F03_ordinary_least_squares.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdfea6e-5bff-4e31-b35a-6ba7a1554a52",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>Prerequisites</b>\n",
    "    \n",
    "- Define a supervised learning problem.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Learning Outcomes</b>\n",
    "    \n",
    "- Apply the supervised learning methodology to a multiple linear regression\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae3c60c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Strengths of the OLS\n",
    "\n",
    "- Simple to use;\n",
    "- Easily interpretable in terms of variances and covariances;\n",
    "- Can outperform fancier nonlinear models for prediction, especially in situations with:\n",
    "  - small training samples,\n",
    "  - low signal-to-noise ratio,\n",
    "  - sparse data.\n",
    "- Expandable to nonlinear transformations of the inputs;\n",
    "- Can be used as a simple reference to learn about machine learning methodologies (supervised learning, in particular)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94862dfd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Linear Model\n",
    "\n",
    "\\begin{equation}\n",
    "f_{\\boldsymbol{\\beta}}(X) = \\underbrace{\\beta_0}_{\\mathrm{intercept}} + \\sum_{j = 1}^p X_j \\beta_j\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad85fa16",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "$X_j$ can come from :\n",
    "- quantitative inputs;\n",
    "- transformations of quantitative inputs, such as log or square;\n",
    "- basis expansions, such as $X_2 = X_1^2$, $X_3 = X_1^3$;\n",
    "- interactions between variables, for example, $X_3 = X_1 \\cdot X_2$;\n",
    "- numeric or \"dummy\" coding of the levels of qualitative inputs. For example, $X_j, j = 1, \\ldots, 5$, such that $X_j = I(G = j)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9392d1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Residual Sum of Squares\n",
    "\n",
    "The sample-mean estimate of the Expected Training Error with Squared Error Loss gives the *Residual Sum of Squares* (RSS) depending on the parameters:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm{RSS}(\\beta)\n",
    "= \\sum_{i = 1}^N \\left(y_i - f(x_i)\\right)^2\n",
    " = \\sum_{i = 1}^N \\left(y_i - \\beta_0 - \\sum_{j = 1}^p x_{ij} \\beta_j\\right)^2.\n",
    "\\end{equation}\n",
    "\n",
    "<img alt=\"Linear fit\" src=\"images/linear_fit_red.svg\" width=\"360\" style=\"float:left\">\n",
    "<img alt=\"Linear fit\" src=\"images/lin_reg_3D.svg\" width=\"400\" style=\"float:right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8487bb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "> ***Question***\n",
    "> - Assume that $f(x) = \\bar{y}$ (the sample mean of the target).\n",
    "The corresponding RSS is called the Total Sum of Squares (TSS).\n",
    "How does the TSS relate to the sample variance $s_Y^2$ of $Y$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267951b5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "The *coefficient of determination* $R^2$ relates to the RSS as such,\n",
    "\n",
    "\\begin{equation}\n",
    "R^2(\\boldsymbol{\\beta}) = 1 - \\frac{\\mathrm{RSS}(\\boldsymbol{\\beta})}{\\mathrm{TSS}}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f429234",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## How to Minimize the RSS?\n",
    "\n",
    "Denote by $\\mathbf{X}$ the $N \\times (p + 1)$ input-data matrix.\n",
    "\n",
    "The 1st column of $\\mathbf{X}$ is associated with the intercept and is given by the $N$-dimensional vector $\\mathbf{1}$ with all elements equal to 1.\n",
    "\n",
    "Then,\n",
    "\\begin{equation}\n",
    "\\mathrm{RSS}(\\beta) = \\left(\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta}\\right)^\\top \\left(\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta}\\right).\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38d8c9c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "> ***Question***\n",
    "> - Show that the following parameter estimate minimizes the RSS.\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{\\boldsymbol{\\beta}} = \\left(\\mathbf{X}^\\top \\mathbf{X}\\right)^{-1} \\left(\\mathbf{X}^\\top \\mathbf{y}\\right)\n",
    "\\end{equation}\n",
    "> - Show that this solution is unique if and only if $\\mathbf{X}^\\top\\mathbf{X}$ is positive definite.\n",
    "> \n",
    ">  *Hint*: We recall that a matrix $\\mathbf{M}$ is positive definite iif for all vector $\\mathbf u \\ne 0$, we have $\\mathbf u^\\top \\mathbf{M} \\mathbf u >0$ .\n",
    "> - When could this condition not be fulfilled?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6d258f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "> ***Question (optional)***\n",
    "> - Express $R^2(\\hat{\\boldsymbol{\\beta}})$ (above) in terms of explained variance.\n",
    "> - Show that $R^2(\\hat{\\boldsymbol{\\beta}})$ is invariant under linear transformations of the target."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be577b50",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>Remark</b>\n",
    "    \n",
    "The formula for the optimal coefficients is in closed form, meaning that it can be directly computed using a finite number of standard operations.\n",
    "    \n",
    "Nonlinear models (e.g. neural networks) will instead require solving numerical problems iteratively with a finite precision.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397f6d19",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Suppose that the inputs $\\mathbf{x}_1, \\ldots, \\mathbf{x}_p$ (the columns of the data matrix $\\mathbf{X}$) are orthogonal; that is $\\mathbf{x}_j^\\top \\mathbf{x}_k = 0$ for all $j \\ne k$.\n",
    "\n",
    "> ***Question***\n",
    "> - Show that $\\hat{\\beta}_j = \\mathbf{x}_j^\\top \\mathbf{y} / (\\mathbf{x}_j^\\top \\mathbf{x}_j)$ for all $j$.\n",
    "> - Interpret these coefficients in terms of correlations and variances.\n",
    "> - How do the inputs influence each other's parameter estimates in the model?\n",
    "> - Find a simple expression of $R^2(\\hat{\\beta})$ in that case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4d0f27",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "We now assume that the target is generated by this model $Y = \\boldsymbol{X} \\boldsymbol{\\beta} + \\epsilon$, where the observations of $\\epsilon$ are *uncorrelated* and with *mean zero* and *constant variance* $\\sigma^2$.\n",
    "\n",
    "> ***Question (optional)***\n",
    "> - Knowing that $\\boldsymbol{X} = \\boldsymbol{x}$, show that the observations of $y$ are uncorrelated, with mean $\\boldsymbol{x}^\\top \\boldsymbol{\\beta}$ and variance $\\sigma^2$.\n",
    "> - Show that $\\mathbb{E}(\\hat{\\boldsymbol{\\beta}} | \\mathbf{X}) = \\boldsymbol{\\beta}$ and $\\mathrm{Var}(\\hat{\\boldsymbol{\\beta}} | \\mathbf{X}) = \\sigma^2 (\\mathbf{X}^\\top \\mathbf{X})^{-1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20391053-6c17-4dc2-a1a7-a2a37a567f1f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>Remark</b>\n",
    "    \n",
    "So, for a linear problem in which the errors are uncorrelated with mean zero and constant variance, the OLS is the Best Linear <b>Unbiaised</b> Estimator (BLUE).\n",
    "\n",
    "This is the content of the [Gaussâ€“Markov theorem](https://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2ef5ab",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Confidence Intervals\n",
    "\n",
    "We now assume that the error $\\epsilon$ is a Gaussian random variable, i.e $\\epsilon \\sim N(0, \\sigma^2)$ and would like to test the null hypothesis that $\\beta_j = 0$.\n",
    "\n",
    "> ***Question (optional)***\n",
    "> - Show that the $1 - 2 \\alpha$ confidence interval for $\\beta_j$ is\n",
    ">\n",
    "> $(\\hat{\\beta}_j - z^{(1 - \\alpha)}_{N - p - 1} \\hat{\\sigma} \\sqrt{v_j}, \\ \\ \\ \\ \\hat{\\beta}_j + z^{(1 - \\alpha)}_{N - p - 1} \\hat{\\sigma} \\sqrt{v_j})$,\n",
    ">\n",
    "> where $v_j = [(\\mathbf{X}^\\top \\mathbf{X})^{-1}]_{jj}$ and $z^{(1 - \\alpha)}_{N - p - 1}$ is the $(1 - \\alpha)$ percentile of $t_{N - p - 1}$ (see [Supplementary Material](appendix_supplementary_matrial.ipynb))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcae794-bb01-4f7c-b6a9-943ead08986c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Bayesian approach\n",
    "\n",
    "In the approach presented above, $\\beta$ is treated as an unknown fixed constant, and the data is treated as random. In the Bayesian approach, we treat the data as fixed (since it is known) and the parameter as random (since it is unknown)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612cfc4b-17cd-4598-9c11-8e458bf303cf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "In the frequentist approach, a key aspect is to define a loss function: the solution to the ordinary least square problem is then obtained by minimizing the loss function.\n",
    "\n",
    "$$y = f (x) + \\epsilon\\, ,$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc34778-f5cb-43d7-b32a-c8481edb46d3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "In the probabilistic approach, we consider\n",
    "\n",
    "$$ y = \\mathbf x^\\top \\mathbf \\theta + \\epsilon\\, , \\quad \\epsilon \\sim \\mathcal N(0,\\sigma^2)$$\n",
    "\n",
    "$$\\Leftrightarrow p(y|\\mathbf x, \\mathbf \\theta) = \\mathcal N( \\mathbf x^\\top \\mathbf \\theta ,\\sigma^2)$$\n",
    "\n",
    "In this problem, we treat $\\theta$ as a variable and search for the ***Maximum likelihood***: find the optimal parameters $\\theta^* \\in \t\\mathbb{R}^D$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eba2ce-79df-4ced-a733-49ac9221c1ed",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "In fact, we have $N$ observations $\\mathcal D = \\{(x_1,y_1), ... (x_N,y_N)\\}$. Let us call $\\mathcal X = \\{ x_1, ... x_N\\}$ the set of input variable and $\\mathcal Y = \\{ y_1, ... y_N\\}$ the corresponding set of output variable.  Then, we want to maximize the likelihood\n",
    "\n",
    "\\begin{equation}\n",
    "p(\\mathcal Y|\\mathcal X,\\mathbf{\\theta})\n",
    "\\end{equation}\n",
    "\n",
    "$$\\theta^* = \\underset{\\theta}{\\mathrm{argmin}}\\  p(\\mathcal Y|\\mathcal X,\\mathbf{\\theta})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40710d50-67ae-44a7-981e-98e3615b5379",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "The natural approach to find $\\theta^*$ is to perform a gradient descent. We are going to use this approach when dealing with neural network or with non-linear functions. For the linear regression, we can get a closed form solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3737c19f-aa45-44f6-b8e7-89afbf19c6b8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "We assume that all observations are independent and identically distributed such that\n",
    "\\begin{equation}\n",
    "p(\\mathcal Y|\\mathcal X,\\mathbf{\\theta}) = \\prod_{n=1}^N p(y_i|x_i,\\mathbf{\\theta})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e221639f-a496-40b1-b849-f02c8baa89c0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Then we use the definition of the normal distribution\n",
    "$$p(y|\\mathbf x, \\mathbf \\theta) = \\mathcal N( \\mathbf x^\\top \\mathbf \\theta ,\\sigma^2) = \\frac {1}{\\sqrt{2\\pi \\sigma^2}} \\exp \\left(-\\frac {(y-\\mathbf x^\\top \\mathbf \\theta)^2}{2\\sigma^2}\\right)$$\n",
    "such that\n",
    "\n",
    "\\begin{equation}\n",
    "\\log p(y_i|x_i,\\mathbf{\\theta}) = - \\frac{1}{2\\sigma^2}(y_i-\\mathbf x_i^\\top \\mathbf \\theta)^2 + K\n",
    "\\end{equation}\n",
    "\n",
    "with $K$ a constant independant of $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc55225f-08d5-42bc-9685-58a4bfd2b1e0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "The negative log likelihood of the loss function is then\n",
    "\n",
    "$$\\mathcal L(\\theta) = \\frac{1}{2\\sigma^2}\\sum_{i=1}^N (y_i-\\mathbf x_i^\\top \\mathbf \\theta)^2$$\n",
    "\n",
    "which we can write in matrix form \n",
    "$$\\mathcal L(\\theta) = \\frac{1}{2\\sigma^2} (\\mathbf y - \\mathbf X \\theta)^\\top (\\mathbf y - \\mathbf X \\theta) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bc82c7-24d7-45ce-8b30-8731bddc96ab",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "> ***Question***\n",
    "> - Show that the solution to this problem is the same as the solution of the frequentist approach\n",
    "> - What is the difference between the two approaches?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5011ed",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Polynomial fit and other basis expansion\n",
    "\n",
    "Sometimes linear regressions are too simple to model a complex interaction (see example in introduction). A more advanced model would be to consider\n",
    "\n",
    "$$y = \\phi^\\top (\\mathbf x) \\theta + \\epsilon$$\n",
    "\n",
    "where $\\phi : \\mathbb R^D \\rightarrow \\mathbb R^K$ is a transformation of the inputs $\\mathbf x$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a058d83-40f5-45c5-bd37-f93a7f57f342",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "For instance, for a polynomial regression of degree $K-1$, we propose for $\\phi$\n",
    "\n",
    "$$\\phi(x) = \\begin{bmatrix}\n",
    "1 \\\\\n",
    "x \\\\\n",
    "x^2\\\\\n",
    "\\vdots\\\\\n",
    "x^{K-1}\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac7d191-fa18-42de-b4ad-751ea8554113",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "> ***Question***\n",
    "> - Write the matrix $\\mathbf \\Phi$ that will replace the matrix $\\mathbf x$ in the loss function.\n",
    "> - Write the solution for the optimal parameters  $\\theta^*$ as a function of $\\mathbf \\Phi$ and $\\mathbf y$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7005fab6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## References\n",
    "\n",
    "- [James, G., Witten, D., Hastie, T., Tibshirani, R., n.d. *An Introduction to Statistical Learning*, 2st ed. Springer, New York, NY.](https://www.statlearning.com/)\n",
    "- Chap. 2, 3 and 7 in [Hastie, T., Tibshirani, R., Friedman, J., 2009. *The Elements of Statistical Learning*, 2nd ed. Springer, New York.](https://doi.org/10.1007/978-0-387-84858-7)\n",
    "- Chap. 5 and 7 in [Wilks, D.S., 2019. *Statistical Methods in the Atmospheric Sciences*, 4th ed. Elsevier, Amsterdam.](https://doi.org/10.1016/C2017-0-03921-6)\n",
    "- [Kevin Murphy *Probabilistic Machine Learning: An Introduction*](https://probml.github.io/pml-book/book1.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e186998",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "***\n",
    "## Credit\n",
    "\n",
    "[//]: # \"This notebook is part of [E4C Interdisciplinary Center - Education](https://gitlab.in2p3.fr/energy4climate/public/education).\"\n",
    "Contributors include Bruno Deremble and Alexis Tantet.\n",
    "Several slides and images are taken from the very good [Scikit-learn course](https://inria.github.io/scikit-learn-mooc/).\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"display: flex; height: 70px\">\n",
    "    \n",
    "<img alt=\"Logo LMD\" src=\"images/logos/logo_lmd.jpg\" style=\"display: inline-block\"/>\n",
    "\n",
    "<img alt=\"Logo IPSL\" src=\"images/logos/logo_ipsl.png\" style=\"display: inline-block\"/>\n",
    "\n",
    "<img alt=\"Logo E4C\" src=\"images/logos/logo_e4c_final.png\" style=\"display: inline-block\"/>\n",
    "\n",
    "<img alt=\"Logo EP\" src=\"images/logos/logo_ep.png\" style=\"display: inline-block\"/>\n",
    "\n",
    "<img alt=\"Logo SU\" src=\"images/logos/logo_su.png\" style=\"display: inline-block\"/>\n",
    "\n",
    "<img alt=\"Logo ENS\" src=\"images/logos/logo_ens.jpg\" style=\"display: inline-block\"/>\n",
    "\n",
    "<img alt=\"Logo CNRS\" src=\"images/logos/logo_cnrs.png\" style=\"display: inline-block\"/>\n",
    "    \n",
    "</div>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<div style=\"display: flex\">\n",
    "    <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0; margin-right: 10px\" src=\"https://i.creativecommons.org/l/by-sa/4.0/88x31.png\" /></a>\n",
    "    <br>This work is licensed under a &nbsp; <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\">Creative Commons Attribution-ShareAlike 4.0 International License</a>.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": false,
   "autoclose": true,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
